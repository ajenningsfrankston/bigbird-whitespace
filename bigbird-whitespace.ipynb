{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PyTorch BigBird with Whitespace\n\nI have added spelling correction. \n\n\nNotes from original notebook:\n\nI modfied Chris Deotte's notebook to feed whitespace to the BigBird model. This allows the model to have more contextual information like paragraph start/end.\n\nOriginal Notebook - https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615\n\nThis notebook is a PyTorch starter notebook for Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. It demonstrates how to train, infer, and submit a model to Kaggle without internet. Currently this notebook uses\n\n* backbone BigBird  (with HuggingFace's head for TokenClassification)\n* NER formulation (with `is_split_into_words=False` tokenization)\n* one fold\n\nBy changing a few lines of code, we can use this notebook to evaluate different PyTorch backbones! And we can run all sorts of other experiments. If we try a backbone that doesn't accept 1024 wide tokens (like BigBird or LongFormer), then we can add a sliding window to train and inference. BigBird is a new SOTA transformer with arXiv paper [here][3] which can accept large token inputs as wide as 4096!\n\nThe model in this notebook uses HuggingFace's `AutoModelForTokenClassification`. If we want a custom head, we could use `AutoModel` and then build our own head. See my TensorFlow notebook [here][2] for an example.\n\nThis notebook uses many code cells from Raghavendrakotala's great notebook [here][1]. Don't forget to upvote Raghavendrakotala's notebook :-)\n\n[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n[2]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n[3]: https://arxiv.org/abs/2007.14062","metadata":{}},{"cell_type":"code","source":"#detect where we are running and adjust accordingly \n\non_kaggle = False\n\nimport os\n\nif os.environ.get('PWD') == '/kaggle/working':\n    on_kaggle = True\n    \nif not on_kaggle: \n    !pip install kaggle\n    import zipfile\n\n    !kaggle competitions download -c feedback-prize-2021 -p ../input/feedback-prize-2021\n\n    !kaggle datasets download -d cdeotte/py-bigbird-v26 -p ../input/py-bigbird-v26\n\n    with zipfile.ZipFile('../input/feedback-prize-2021/feedback-prize-2021.zip','r') as myzip:\n        myzip.extractall('../input/feedback-prize-2021')\n\n    with zipfile.ZipFile('../input/py-bigbird-v26/py-bigbird-v26.zip','r') as myzip:\n        myzip.extractall('../input/py-bigbird-v26')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:57:57.618869Z","iopub.execute_input":"2022-02-02T02:57:57.619758Z","iopub.status.idle":"2022-02-02T02:57:57.653325Z","shell.execute_reply.started":"2022-02-02T02:57:57.619631Z","shell.execute_reply":"2022-02-02T02:57:57.652654Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Configuration\nThis notebook can either train a new model or load a previously trained model (made from previous notebook version). Furthermore, this notebook can either create new NER labels or load existing NER labels (made from previous notebook version). In this notebook version, we will load model and load NER labels.\n\nAlso this notebook can load huggingface stuff (like tokenizers) from a Kaggle dataset, or download it from internet. (If it downloads from internet, you can then put it in a Kaggle dataset, so next time you can turn internet off).","metadata":{}},{"cell_type":"code","source":"if not on_kaggle:\n    !pip install sentencepiece\n    !pip install transformers","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:57:57.654887Z","iopub.execute_input":"2022-02-02T02:57:57.655201Z","iopub.status.idle":"2022-02-02T02:57:57.662101Z","shell.execute_reply.started":"2022-02-02T02:57:57.655164Z","shell.execute_reply":"2022-02-02T02:57:57.661413Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING MODEL WEIGHTS\nVER=27\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\nLOAD_TOKENS_FROM = '../input/py-bigbird-v26'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\nLOAD_MODEL_FROM = None\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = None\nif DOWNLOADED_MODEL_PATH is None:\n    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'google/bigbird-roberta-base'","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:57:57.663782Z","iopub.execute_input":"2022-02-02T02:57:57.664131Z","iopub.status.idle":"2022-02-02T02:57:57.670464Z","shell.execute_reply.started":"2022-02-02T02:57:57.664077Z","shell.execute_reply":"2022-02-02T02:57:57.669410Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\nconfig = {'model_name': MODEL_NAME,   \n         'max_length': 1024,\n         'train_batch_size':4,\n         'valid_batch_size':4,\n         'epochs':5,\n         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n         'max_grad_norm':10,\n         'device': 'cuda' if cuda.is_available() else 'cpu'}\n\n# THIS WILL COMPUTE VAL SCORE DURING COMMIT BUT NOT DURING SUBMIT\nCOMPUTE_VAL_SCORE = True\nif len( os.listdir('../input/feedback-prize-2021/test') )>5:\n      COMPUTE_VAL_SCORE = False","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:57:57.671729Z","iopub.execute_input":"2022-02-02T02:57:57.672077Z","iopub.status.idle":"2022-02-02T02:57:59.221696Z","shell.execute_reply.started":"2022-02-02T02:57:57.672041Z","shell.execute_reply":"2022-02-02T02:57:59.220908Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# How To Submit PyTorch Without Internet\nMany people ask me, how do I submit PyTorch models without internet? With HuggingFace Transformer, it's easy. Just download the following 3 things (1) model weights, (2) tokenizer files, (3) config file, and upload them to a Kaggle dataset. Below shows code how to get the files from HuggingFace for Google's BigBird-base. But this same code can download any transformer, like for example roberta-base.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoConfig,  AutoModelForTokenClassification\nif DOWNLOADED_MODEL_PATH == 'model':\n    os.mkdir('model')\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n    tokenizer.save_pretrained('model')\n\n    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n    config_model.num_labels = 15\n    config_model.save_pretrained('model')\n\n    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n                                                               config=config_model)\n    backbone.save_pretrained('model')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:57:59.224255Z","iopub.execute_input":"2022-02-02T02:57:59.224521Z","iopub.status.idle":"2022-02-02T02:58:23.671078Z","shell.execute_reply.started":"2022-02-02T02:57:59.224486Z","shell.execute_reply":"2022-02-02T02:58:23.670224Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4175af20f5744ff9bc81ec3838832110"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90b6d38b85a49dbb716b098427d0812"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/826k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec71a24c897844028034943ad73b09b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/775 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb14b12725946b8b4757503affb9d3a"}},"metadata":{}},{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/489M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fd9cbf26cca463193395075acb191a6"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Data and Libraries\nIn addition to loading the train dataframe, we will load all the train and text files and save them in a dataframe.","metadata":{}},{"cell_type":"code","source":"import numpy as np, os \nfrom scipy import stats\nimport pandas as pd, gc \nfrom tqdm import tqdm\n\nif not on_kaggle:\n    !pip install sklearn\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom sklearn.metrics import accuracy_score","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":3.432226,"end_time":"2021-12-21T12:12:07.26275","exception":false,"start_time":"2021-12-21T12:12:03.830524","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:58:23.672335Z","iopub.execute_input":"2022-02-02T02:58:23.672598Z","iopub.status.idle":"2022-02-02T02:58:24.204895Z","shell.execute_reply.started":"2022-02-02T02:58:23.672565Z","shell.execute_reply":"2022-02-02T02:58:24.204063Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\nprint( train_df.shape )\ntrain_df.head()","metadata":{"papermill":{"duration":1.866495,"end_time":"2021-12-21T12:12:09.158087","exception":false,"start_time":"2021-12-21T12:12:07.291592","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:58:24.206318Z","iopub.execute_input":"2022-02-02T02:58:24.206617Z","iopub.status.idle":"2022-02-02T02:58:26.260819Z","shell.execute_reply.started":"2022-02-02T02:58:24.206578Z","shell.execute_reply":"2022-02-02T02:58:26.260201Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(144293, 8)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"             id  discourse_id  discourse_start  discourse_end  \\\n0  423A1CA112E2  1.622628e+12              8.0          229.0   \n1  423A1CA112E2  1.622628e+12            230.0          312.0   \n2  423A1CA112E2  1.622628e+12            313.0          401.0   \n3  423A1CA112E2  1.622628e+12            402.0          758.0   \n4  423A1CA112E2  1.622628e+12            759.0          886.0   \n\n                                      discourse_text discourse_type  \\\n0  Modern humans today are always on their phone....           Lead   \n1  They are some really bad consequences when stu...       Position   \n2  Some certain areas in the United States ban ph...       Evidence   \n3  When people have phones, they know about certa...       Evidence   \n4  Driving is one of the way how to get around. P...          Claim   \n\n  discourse_type_num                                   predictionstring  \n0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_id</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_type_num</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>8.0</td>\n      <td>229.0</td>\n      <td>Modern humans today are always on their phone....</td>\n      <td>Lead</td>\n      <td>Lead 1</td>\n      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>230.0</td>\n      <td>312.0</td>\n      <td>They are some really bad consequences when stu...</td>\n      <td>Position</td>\n      <td>Position 1</td>\n      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>313.0</td>\n      <td>401.0</td>\n      <td>Some certain areas in the United States ban ph...</td>\n      <td>Evidence</td>\n      <td>Evidence 1</td>\n      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>402.0</td>\n      <td>758.0</td>\n      <td>When people have phones, they know about certa...</td>\n      <td>Evidence</td>\n      <td>Evidence 2</td>\n      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>759.0</td>\n      <td>886.0</td>\n      <td>Driving is one of the way how to get around. P...</td>\n      <td>Claim</td>\n      <td>Claim 1</td>\n      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install autocorrect\nfrom autocorrect import Speller\nspell = Speller(fast=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:58:26.264327Z","iopub.execute_input":"2022-02-02T02:58:26.266416Z","iopub.status.idle":"2022-02-02T02:58:40.365318Z","shell.execute_reply.started":"2022-02-02T02:58:26.266379Z","shell.execute_reply":"2022-02-02T02:58:40.364325Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting autocorrect\n  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n     |████████████████████████████████| 622 kB 889 kB/s            \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622381 sha256=95a0e048ef84f0725497041c15254d82d052f29e956788bb06f052395c322716\n  Stored in directory: /root/.cache/pip/wheels/54/d4/37/8244101ad50b0f7d9bffd93ce58ed7991ee1753b290923934b\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntest_names, test_texts = [], []\nfor f in list(os.listdir('../input/feedback-prize-2021/test')):\n    test_names.append(f.replace('.txt', ''))\n    text = open('../input/feedback-prize-2021/test/' + f, 'r').read()\n    text = spell(text)\n    test_texts.append(text)\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\ntest_texts.head()","metadata":{"papermill":{"duration":0.083228,"end_time":"2021-12-21T12:12:09.396487","exception":false,"start_time":"2021-12-21T12:12:09.313259","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:58:40.367223Z","iopub.execute_input":"2022-02-02T02:58:40.367629Z","iopub.status.idle":"2022-02-02T02:58:40.442008Z","shell.execute_reply.started":"2022-02-02T02:58:40.367580Z","shell.execute_reply":"2022-02-02T02:58:40.441354Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"             id                                               text\n0  0FB0700DAF44  During a group project, have you ever asked a ...\n1  D72CB1C11673  Making choices in life can be very difficult. ...\n2  18409261F5C2  80% of Americans believe seeking multiple opin...\n3  DF920E0A7337  Have you ever asked more than one person for h...\n4  D46BCB48440A  When people ask for advice,they sometimes talk...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0FB0700DAF44</td>\n      <td>During a group project, have you ever asked a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D72CB1C11673</td>\n      <td>Making choices in life can be very difficult. ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18409261F5C2</td>\n      <td>80% of Americans believe seeking multiple opin...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DF920E0A7337</td>\n      <td>Have you ever asked more than one person for h...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D46BCB48440A</td>\n      <td>When people ask for advice,they sometimes talk...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntrain_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    train_names.append(f.replace('.txt', ''))\n    text = open('../input/feedback-prize-2021/train/' + f, 'r').read()\n    text = spell(text)\n    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\ntrain_text_df = pd.DataFrame({'id': train_names, 'text': train_texts})\ntrain_text_df.head()","metadata":{"papermill":{"duration":38.695201,"end_time":"2021-12-21T12:12:48.120383","exception":false,"start_time":"2021-12-21T12:12:09.425182","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T02:58:40.443280Z","iopub.execute_input":"2022-02-02T02:58:40.443652Z","iopub.status.idle":"2022-02-02T03:01:08.608381Z","shell.execute_reply.started":"2022-02-02T02:58:40.443618Z","shell.execute_reply":"2022-02-02T03:01:08.607601Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 15594/15594 [02:27<00:00, 105.51it/s]\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"             id                                               text\n0  62C57C524CD2  I think we should be able to play in a sport i...\n1  80667AD3FFD8  Some schools require summer projects for stude...\n2  21868C40B94F  Driverless cars have been argued and talked ab...\n3  87A6EF3113C6  The author of \"The Challenge of Exploring Venu...\n4  24687D08CFDA  Wow, from the mar really look like humans face...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>62C57C524CD2</td>\n      <td>I think we should be able to play in a sport i...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80667AD3FFD8</td>\n      <td>Some schools require summer projects for stude...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21868C40B94F</td>\n      <td>Driverless cars have been argued and talked ab...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>87A6EF3113C6</td>\n      <td>The author of \"The Challenge of Exploring Venu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24687D08CFDA</td>\n      <td>Wow, from the mar really look like humans face...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Convert Train Text to NER Labels\nWe will now convert all text words into NER labels and save in a dataframe.","metadata":{"papermill":{"duration":0.123678,"end_time":"2021-12-21T12:12:48.368476","exception":false,"start_time":"2021-12-21T12:12:48.244798","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if not LOAD_TOKENS_FROM:\n    all_entities = []\n    for ii,i in enumerate(train_text_df.iterrows()):\n        if ii%100==0: print(ii,', ',end='')\n        total = i[1]['text'].split().__len__()\n        entities = [\"O\"]*total\n        for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n            discourse = j[1]['discourse_type']\n            list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n            entities[list_ix[0]] = f\"B-{discourse}\"\n            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n        all_entities.append(entities)\n    train_text_df['entities'] = all_entities\n    train_text_df.to_csv('train_NER.csv',index=False)\n    \nelse:\n    from ast import literal_eval\n    train_text_df = pd.read_csv(f'{LOAD_TOKENS_FROM}/train_NER.csv')\n    # pandas saves lists as string, we must convert back\n    train_text_df.entities = train_text_df.entities.apply(lambda x: literal_eval(x) )\n    \nprint( train_text_df.shape )\ntrain_text_df.head()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:08.610058Z","iopub.execute_input":"2022-02-02T03:01:08.610559Z","iopub.status.idle":"2022-02-02T03:01:21.481367Z","shell.execute_reply.started":"2022-02-02T03:01:08.610517Z","shell.execute_reply":"2022-02-02T03:01:21.480676Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(15594, 3)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"             id                                               text  \\\n0  E1FA876D6E6C  Dear Senator,\\n\\nI am writting this letter to ...   \n1  8AC1D6E165CD  Dear Principal, I believe in policy 2. Kids ar...   \n2  45EF6A4EDB1A  Summer projects are no fun, but they are a gre...   \n3  B0070361406D  The author who wrote \"The challenge of Explori...   \n4  839F4F7F7DD7  Our school systems have seen many changes as t...   \n\n                                            entities  \n0  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...  \n1  [O, O, B-Position, I-Position, I-Position, I-P...  \n2  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n4  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>entities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E1FA876D6E6C</td>\n      <td>Dear Senator,\\n\\nI am writting this letter to ...</td>\n      <td>[O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8AC1D6E165CD</td>\n      <td>Dear Principal, I believe in policy 2. Kids ar...</td>\n      <td>[O, O, B-Position, I-Position, I-Position, I-P...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>45EF6A4EDB1A</td>\n      <td>Summer projects are no fun, but they are a gre...</td>\n      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B0070361406D</td>\n      <td>The author who wrote \"The challenge of Explori...</td>\n      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>839F4F7F7DD7</td>\n      <td>Our school systems have seen many changes as t...</td>\n      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}","metadata":{"papermill":{"duration":0.940609,"end_time":"2021-12-21T12:18:50.456125","exception":false,"start_time":"2021-12-21T12:18:49.515516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.482840Z","iopub.execute_input":"2022-02-02T03:01:21.483332Z","iopub.status.idle":"2022-02-02T03:01:21.489245Z","shell.execute_reply.started":"2022-02-02T03:01:21.483295Z","shell.execute_reply":"2022-02-02T03:01:21.488584Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"labels_to_ids","metadata":{"papermill":{"duration":0.994404,"end_time":"2021-12-21T12:18:52.798977","exception":false,"start_time":"2021-12-21T12:18:51.804573","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.490585Z","iopub.execute_input":"2022-02-02T03:01:21.490870Z","iopub.status.idle":"2022-02-02T03:01:21.504755Z","shell.execute_reply.started":"2022-02-02T03:01:21.490813Z","shell.execute_reply":"2022-02-02T03:01:21.504034Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'O': 0,\n 'B-Lead': 1,\n 'I-Lead': 2,\n 'B-Position': 3,\n 'I-Position': 4,\n 'B-Claim': 5,\n 'I-Claim': 6,\n 'B-Counterclaim': 7,\n 'I-Counterclaim': 8,\n 'B-Rebuttal': 9,\n 'I-Rebuttal': 10,\n 'B-Evidence': 11,\n 'I-Evidence': 12,\n 'B-Concluding Statement': 13,\n 'I-Concluding Statement': 14}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Define the dataset function\nBelow is our PyTorch dataset function. It always outputs tokens and attention. During training it also provides labels. And during inference it also provides word ids to help convert token predictions into word predictions.\n\nNote that we use `text.split()` and `is_split_into_words=True` when we convert train text to labeled train tokens. This is how the HugglingFace tutorial does it. However, this removes characters like `\\n` new paragraph. If you want your model to see new paragraphs, then we need to map words to tokens ourselves using `return_offsets_mapping=True`. See my TensorFlow notebook [here][1] for an example.\n\nSome of the following code comes from the example at HuggingFace [here][2]. However I think the code at that link is wrong. The HuggingFace original code is [here][3]. With the flag `LABEL_ALL` we can either label just the first subword token (when one word has more than one subword token). Or we can label all the subword tokens (with the word's label). In this notebook version, we label all the tokens. There is a Kaggle discussion [here][4]\n\n[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n[2]: https://huggingface.co/docs/transformers/custom_datasets#tok_ner\n[3]: https://github.com/huggingface/transformers/blob/86b40073e9aee6959c8c85fcba89e47b432c4f4d/examples/pytorch/token-classification/run_ner.py#L371\n[4]: https://www.kaggle.com/c/feedback-prize-2021/discussion/296713","metadata":{"papermill":{"duration":1.001889,"end_time":"2021-12-21T12:18:54.981896","exception":false,"start_time":"2021-12-21T12:18:53.980007","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Return an array that maps character index to index of word in list of split() words\ndef split_mapping(unsplit):\n    splt = unsplit.split()\n    offset_to_wordidx = np.full(len(unsplit),-1)\n    txt_ptr = 0\n    for split_index, full_word in enumerate(splt):\n        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n            txt_ptr += 1\n        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n        txt_ptr += len(full_word)\n    return offset_to_wordidx","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.509880Z","iopub.execute_input":"2022-02-02T03:01:21.510231Z","iopub.status.idle":"2022-02-02T03:01:21.516157Z","shell.execute_reply.started":"2022-02-02T03:01:21.510201Z","shell.execute_reply":"2022-02-02T03:01:21.515172Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class dataset(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.get_wids = get_wids # for validation\n\n  def __getitem__(self, index):\n        # GET TEXT AND WORD LABELS \n        text = self.data.text[index]        \n        word_labels = self.data.entities[index] if not self.get_wids else None\n\n        # TOKENIZE TEXT\n        encoding = self.tokenizer(text,\n                             return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        word_ids = encoding.word_ids()  \n        split_word_ids = np.full(len(word_ids),-1)\n        offset_to_wordidx = split_mapping(text)\n        offsets = encoding['offset_mapping']\n        \n        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n        label_ids = []\n        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n            \n            if word_idx is None:\n                if not self.get_wids: label_ids.append(-100)\n            else:\n                if offsets[token_idx] != (0,0):\n                    #Choose the split word that shares the most characters with the token if any\n                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n                    \n                    if split_index != -1: \n                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n                        split_word_ids[token_idx] = split_index\n                    else:\n                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n                            if not self.get_wids: label_ids.append(label_ids[-1])\n                        else:\n                            if not self.get_wids: label_ids.append(-100)\n                else:\n                    if not self.get_wids: label_ids.append(-100)\n        \n        encoding['labels'] = list(reversed(label_ids))\n\n        # CONVERT TO TORCH TENSORS\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        if self.get_wids: \n            item['wids'] = torch.as_tensor(split_word_ids)\n        \n        return item\n\n  def __len__(self):\n        return self.len","metadata":{"papermill":{"duration":0.934726,"end_time":"2021-12-21T12:18:56.852259","exception":false,"start_time":"2021-12-21T12:18:55.917533","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.517861Z","iopub.execute_input":"2022-02-02T03:01:21.518203Z","iopub.status.idle":"2022-02-02T03:01:21.534565Z","shell.execute_reply.started":"2022-02-02T03:01:21.518169Z","shell.execute_reply":"2022-02-02T03:01:21.533895Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Create Train and Validation Dataloaders\nWe will use the same train and validation subsets as my TensorFlow notebook [here][1]. Then we can compare results. And/or experiment with ensembling the validation fold predictions.\n\n[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617","metadata":{"papermill":{"duration":0.936225,"end_time":"2021-12-21T12:19:08.206923","exception":false,"start_time":"2021-12-21T12:19:07.270698","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# CHOOSE VALIDATION INDEXES (that match my TF notebook)\nIDS = train_df.id.unique()\nprint('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n\n# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.537161Z","iopub.execute_input":"2022-02-02T03:01:21.537631Z","iopub.status.idle":"2022-02-02T03:01:21.567413Z","shell.execute_reply.started":"2022-02-02T03:01:21.537592Z","shell.execute_reply":"2022-02-02T03:01:21.566715Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"There are 15594 train texts. We will split 90% 10% for validation.\n","output_type":"stream"}]},{"cell_type":"code","source":"# CREATE TRAIN SUBSET AND VALID SUBSET\ndata = train_text_df[['id','text', 'entities']]\ntrain_dataset = data.loc[data['id'].isin(IDS[train_idx]),['text', 'entities']].reset_index(drop=True)\ntest_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \ntraining_set = dataset(train_dataset, tokenizer, config['max_length'], False)\ntesting_set = dataset(test_dataset, tokenizer, config['max_length'], True)","metadata":{"papermill":{"duration":0.953973,"end_time":"2021-12-21T12:19:10.088215","exception":false,"start_time":"2021-12-21T12:19:09.134242","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.568717Z","iopub.execute_input":"2022-02-02T03:01:21.568982Z","iopub.status.idle":"2022-02-02T03:01:21.703985Z","shell.execute_reply.started":"2022-02-02T03:01:21.568945Z","shell.execute_reply":"2022-02-02T03:01:21.703082Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"FULL Dataset: (15594, 3)\nTRAIN Dataset: (14034, 2)\nTEST Dataset: (1560, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN DATASET AND VALID DATASET\ntrain_params = {'batch_size': config['train_batch_size'],\n                'shuffle': True,\n                'num_workers': 2,\n                'pin_memory':True\n                }\n\ntest_params = {'batch_size': config['valid_batch_size'],\n                'shuffle': False,\n                'num_workers': 2,\n                'pin_memory':True\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)\n\n# TEST DATASET\ntest_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\ntest_texts_loader = DataLoader(test_texts_set, **test_params)","metadata":{"papermill":{"duration":0.955464,"end_time":"2021-12-21T12:19:12.022567","exception":false,"start_time":"2021-12-21T12:19:11.067103","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.705457Z","iopub.execute_input":"2022-02-02T03:01:21.705791Z","iopub.status.idle":"2022-02-02T03:01:21.712472Z","shell.execute_reply.started":"2022-02-02T03:01:21.705753Z","shell.execute_reply":"2022-02-02T03:01:21.711433Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Train Model\nThe PyTorch train function is taken from Raghavendrakotala's great notebook [here][1]. I assume it uses a masked loss which avoids computing loss when target is `-100`. If not, we need to update this.\n\nIn Kaggle notebooks, we will train our model for 5 epochs `batch_size=4` with Adam optimizer and learning rates `LR = [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7]`. The loaded model was trained offline with `batch_size=8` and `LR = [5e-5, 5e-5, 5e-6, 5e-6, 5e-7]`. (Note the learning rate changes `e-5`, `e-6`, and `e-7`). Using `batch_size=4` will probably achieve a better validation score than `batch_size=8`, but I haven't tried yet.\n\n[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ndef train(epoch,device):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    #tr_preds, tr_labels = [], []\n    \n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n    \n        if not on_kaggle:\n            ids = batch['input_ids'].to(device, dtype = torch.long)\n            mask = batch['attention_mask'].to(device, dtype = torch.long)\n            labels = batch['labels'].to(device, dtype = torch.long)\n        else:\n            ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n            mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n            labels = batch['labels'].to(config['device'], dtype = torch.long)\n\n        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n                               return_dict=False)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n        if idx % 10==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        #tr_labels.extend(labels)\n        #tr_preds.extend(predictions)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=config['max_grad_norm']\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","metadata":{"papermill":{"duration":1.00345,"end_time":"2021-12-21T12:19:31.294225","exception":false,"start_time":"2021-12-21T12:19:30.290775","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.713983Z","iopub.execute_input":"2022-02-02T03:01:21.714489Z","iopub.status.idle":"2022-02-02T03:01:21.729336Z","shell.execute_reply.started":"2022-02-02T03:01:21.714353Z","shell.execute_reply":"2022-02-02T03:01:21.728560Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"torch.device(0)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.731593Z","iopub.execute_input":"2022-02-02T03:01:21.731779Z","iopub.status.idle":"2022-02-02T03:01:21.744912Z","shell.execute_reply.started":"2022-02-02T03:01:21.731757Z","shell.execute_reply":"2022-02-02T03:01:21.744142Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"# CREATE MODEL\nconfig_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \nmodel = AutoModelForTokenClassification.from_pretrained(\n                   DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\ndevice = torch.device(0)\nif on_kaggle:\n    model.to(config['device'])\nelse:\n    model.to(device)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:21.746819Z","iopub.execute_input":"2022-02-02T03:01:21.747439Z","iopub.status.idle":"2022-02-02T03:01:27.603298Z","shell.execute_reply.started":"2022-02-02T03:01:21.747404Z","shell.execute_reply":"2022-02-02T03:01:27.602518Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# LOOP TO TRAIN MODEL (or load model)\nif not LOAD_MODEL_FROM:\n    for epoch in range(config['epochs']):\n        \n        print(f\"### Training epoch: {epoch + 1}\")\n        for g in optimizer.param_groups: \n            g['lr'] = config['learning_rates'][epoch]\n        lr = optimizer.param_groups[0]['lr']\n        print(f'### LR = {lr}\\n')\n        \n        train(epoch,device)\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    torch.save(model.state_dict(), f'bigbird_v{VER}.pt')\nelse:\n    model.load_state_dict(torch.load(f'{LOAD_MODEL_FROM}/bigbird_v{VER}.pt'))\n    print('Model loaded.')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:01:27.604568Z","iopub.execute_input":"2022-02-02T03:01:27.604832Z","iopub.status.idle":"2022-02-02T03:15:51.198189Z","shell.execute_reply.started":"2022-02-02T03:01:27.604798Z","shell.execute_reply":"2022-02-02T03:15:51.196419Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"### Training epoch: 1\n### LR = 2.5e-05\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\nTo keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\n  return torch.floor_divide(self, other)\n","output_type":"stream"},{"name":"stdout","text":"Training loss after 0000 training steps: 2.6704745292663574\nTraining loss after 0010 training steps: 2.028693524273959\nTraining loss after 0020 training steps: 1.8694528057461692\nTraining loss after 0030 training steps: 1.7801441531027518\nTraining loss after 0040 training steps: 1.6805829449397762\nTraining loss after 0050 training steps: 1.6048724347469854\nTraining loss after 0060 training steps: 1.529927101291594\nTraining loss after 0070 training steps: 1.4989693911982254\nTraining loss after 0080 training steps: 1.4656972193423612\nTraining loss after 0090 training steps: 1.418984313587566\nTraining loss after 0100 training steps: 1.3896579659811341\nTraining loss after 0110 training steps: 1.3651059533024694\nTraining loss after 0120 training steps: 1.3411081275664085\nTraining loss after 0130 training steps: 1.3118184841316165\nTraining loss after 0140 training steps: 1.2781366452257683\nTraining loss after 0150 training steps: 1.258978548823603\nTraining loss after 0160 training steps: 1.2428393293611752\nTraining loss after 0170 training steps: 1.2289459750666256\nTraining loss after 0180 training steps: 1.2105635888668713\nTraining loss after 0190 training steps: 1.1867616006529145\nTraining loss after 0200 training steps: 1.1800242637223866\nTraining loss after 0210 training steps: 1.169164211405397\nTraining loss after 0220 training steps: 1.159590061029158\nTraining loss after 0230 training steps: 1.147848789181028\nTraining loss after 0240 training steps: 1.142931294762742\nTraining loss after 0250 training steps: 1.1322749815139164\nTraining loss after 0260 training steps: 1.1213686525593316\nTraining loss after 0270 training steps: 1.112875151678205\nTraining loss after 0280 training steps: 1.1024325661811964\nTraining loss after 0290 training steps: 1.0959089435252947\nTraining loss after 0300 training steps: 1.0893839665029532\nTraining loss after 0310 training steps: 1.0864151361670908\nTraining loss after 0320 training steps: 1.0840307195609975\nTraining loss after 0330 training steps: 1.0750658368055914\nTraining loss after 0340 training steps: 1.0645136779004878\nTraining loss after 0350 training steps: 1.0594606774824638\nTraining loss after 0360 training steps: 1.0498609006239767\nTraining loss after 0370 training steps: 1.043474251770909\nTraining loss after 0380 training steps: 1.0372802235792316\nTraining loss after 0390 training steps: 1.0297162762230925\nTraining loss after 0400 training steps: 1.0224460190222449\nTraining loss after 0410 training steps: 1.017910875180632\nTraining loss after 0420 training steps: 1.0144480645939744\nTraining loss after 0430 training steps: 1.0112411755281923\nTraining loss after 0440 training steps: 1.0098817223459144\nTraining loss after 0450 training steps: 1.004243891677941\nTraining loss after 0460 training steps: 1.0006071866980861\nTraining loss after 0470 training steps: 0.9961845941224675\nTraining loss after 0480 training steps: 0.9907204069491483\nTraining loss after 0490 training steps: 0.9845364960536451\nTraining loss after 0500 training steps: 0.9810809491875166\nTraining loss after 0510 training steps: 0.9778123257678082\nTraining loss after 0520 training steps: 0.9743401734247775\nTraining loss after 0530 training steps: 0.9694261694021817\nTraining loss after 0540 training steps: 0.9668011528176433\nTraining loss after 0550 training steps: 0.9625301334581011\nTraining loss after 0560 training steps: 0.9589915168795357\nTraining loss after 0570 training steps: 0.9551465249834044\nTraining loss after 0580 training steps: 0.9508267235837993\nTraining loss after 0590 training steps: 0.9467338690701274\nTraining loss after 0600 training steps: 0.9426428719784773\nTraining loss after 0610 training steps: 0.9377283235911261\nTraining loss after 0620 training steps: 0.9353750034710827\nTraining loss after 0630 training steps: 0.9323364242132038\nTraining loss after 0640 training steps: 0.9299659379837851\nTraining loss after 0650 training steps: 0.9273068790428466\nTraining loss after 0660 training steps: 0.9250560741471451\nTraining loss after 0670 training steps: 0.9220835947599567\nTraining loss after 0680 training steps: 0.9182367529638014\nTraining loss after 0690 training steps: 0.9169217762139703\nTraining loss after 0700 training steps: 0.9175327073320343\nTraining loss after 0710 training steps: 0.9165110104362337\nTraining loss after 0720 training steps: 0.9133330271908711\nTraining loss after 0730 training steps: 0.9115260289077394\nTraining loss after 0740 training steps: 0.9087555576191257\nTraining loss after 0750 training steps: 0.9056437542530573\nTraining loss after 0760 training steps: 0.9028777148031217\nTraining loss after 0770 training steps: 0.9003468753453513\nTraining loss after 0780 training steps: 0.8965309316461737\nTraining loss after 0790 training steps: 0.8930351091395738\nTraining loss after 0800 training steps: 0.8914990880516138\nTraining loss after 0810 training steps: 0.8887895056851548\nTraining loss after 0820 training steps: 0.8864893333194607\nTraining loss after 0830 training steps: 0.8858853965531475\nTraining loss after 0840 training steps: 0.8836242673867665\nTraining loss after 0850 training steps: 0.8813484349626212\nTraining loss after 0860 training steps: 0.880340916873686\nTraining loss after 0870 training steps: 0.8807943382917398\nTraining loss after 0880 training steps: 0.8783571127797906\nTraining loss after 0890 training steps: 0.8754461218010295\nTraining loss after 0900 training steps: 0.8726023833508232\nTraining loss after 0910 training steps: 0.8710562201415145\nTraining loss after 0920 training steps: 0.8677496441541873\nTraining loss after 0930 training steps: 0.865528346132387\nTraining loss after 0940 training steps: 0.8632492514586474\nTraining loss after 0950 training steps: 0.8618076434897575\nTraining loss after 0960 training steps: 0.8604923833111199\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/828594373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'### LR = {lr}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/2659778735.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Inference and Validation Code\nWe will infer in batches using our data loader which is faster than inferring one text at a time with a for-loop. The metric code is taken from Rob Mulla's great notebook [here][2]. Our model achieves validation F1 score 0.615! \n\nDuring inference our model will make predictions for each subword token. Some single words consist of multiple subword tokens. In the code below, we use a word's first subword token prediction as the label for the entire word. We can try other approaches, like averaging all subword predictions or taking `B` labels before `I` labels etc.\n\n[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch","metadata":{}},{"cell_type":"code","source":"def inference(batch):\n                \n    # MOVE BATCH TO GPU AND INFER\n    ids = batch[\"input_ids\"].to(config['device'])\n    mask = batch[\"attention_mask\"].to(config['device'])\n    outputs = model(ids, attention_mask=mask, return_dict=False)\n    all_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n    \n    # INTERATE THROUGH EACH TEXT AND GET PRED\n    predictions = []\n    for k,text_preds in enumerate(all_preds):\n        token_preds = [ids_to_labels[i] for i in text_preds]\n\n        prediction = []\n        word_ids = batch['wids'][k].numpy()\n        previous_word_idx = -1\n        for idx,word_idx in enumerate(word_ids):                            \n            if word_idx == -1:\n                pass\n            elif word_idx != previous_word_idx:              \n                prediction.append(token_preds[idx])\n                previous_word_idx = word_idx\n        predictions.append(prediction)\n    \n    return predictions","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:15:51.199163Z","iopub.status.idle":"2022-02-02T03:15:51.199456Z","shell.execute_reply.started":"2022-02-02T03:15:51.199295Z","shell.execute_reply":"2022-02-02T03:15:51.199317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/zzy990106/pytorch-ner-infer\n# code has been modified from original\ndef get_predictions(df=test_dataset, loader=testing_loader):\n    \n    # put model in training mode\n    model.eval()\n    \n    # GET WORD LABEL PREDICTIONS\n    y_pred2 = []\n    for batch in loader:\n        labels = inference(batch)\n        y_pred2.extend(labels)\n\n    final_preds2 = []\n    for i in range(len(df)):\n\n        idx = df.id.values[i]\n        #pred = [x.replace('B-','').replace('I-','') for x in y_pred2[i]]\n        pred = y_pred2[i] # Leave \"B\" and \"I\"\n        preds = []\n        j = 0\n        while j < len(pred):\n            cls = pred[j]\n            # The commented out line below appears to be a bug.\n#             if cls == 'O': j += 1\n            if cls == 'O': pass\n            else: cls = cls.replace('B','I') # spans start with B\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n            \n            if cls != 'O' and cls != '' and end - j > 7:\n                final_preds2.append((idx, cls.replace('I-',''),\n                                     ' '.join(map(str, list(range(j, end))))))\n        \n            j = end\n        \n    oof = pd.DataFrame(final_preds2)\n    oof.columns = ['id','class','predictionstring']\n\n    return oof","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:15:51.200914Z","iopub.status.idle":"2022-02-02T03:15:51.201426Z","shell.execute_reply.started":"2022-02-02T03:15:51.201201Z","shell.execute_reply":"2022-02-02T03:15:51.201222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:15:51.202456Z","iopub.status.idle":"2022-02-02T03:15:51.203022Z","shell.execute_reply.started":"2022-02-02T03:15:51.202776Z","shell.execute_reply":"2022-02-02T03:15:51.202801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_VAL_SCORE: # note this doesn't run during submit\n    # VALID TARGETS\n    valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n\n    # OOF PREDICTIONS\n    oof = get_predictions(test_dataset, testing_loader)\n\n    # COMPUTE F1 SCORE\n    f1s = []\n    CLASSES = oof['class'].unique()\n    print()\n    for c in CLASSES:\n        pred_df = oof.loc[oof['class']==c].copy()\n        gt_df = valid.loc[valid['discourse_type']==c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        print(c,f1)\n        f1s.append(f1)\n    print()\n    print('Overall',np.mean(f1s))\n    print()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:15:51.204380Z","iopub.status.idle":"2022-02-02T03:15:51.205161Z","shell.execute_reply.started":"2022-02-02T03:15:51.204923Z","shell.execute_reply":"2022-02-02T03:15:51.204952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test Data and Write Submission CSV\nWe will now infer the test data and write submission CSV","metadata":{"papermill":{"duration":1.170872,"end_time":"2021-12-21T12:58:34.316729","exception":false,"start_time":"2021-12-21T12:58:33.145857","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub = get_predictions(test_texts, test_texts_loader)\nsub.head()","metadata":{"papermill":{"duration":0.998396,"end_time":"2021-12-21T12:58:50.260737","exception":false,"start_time":"2021-12-21T12:58:49.262341","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:15:51.206344Z","iopub.status.idle":"2022-02-02T03:15:51.207107Z","shell.execute_reply.started":"2022-02-02T03:15:51.206873Z","shell.execute_reply":"2022-02-02T03:15:51.206904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"papermill":{"duration":1.020359,"end_time":"2021-12-21T12:58:54.413788","exception":false,"start_time":"2021-12-21T12:58:53.393429","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-02T03:15:51.208266Z","iopub.status.idle":"2022-02-02T03:15:51.209045Z","shell.execute_reply.started":"2022-02-02T03:15:51.208786Z","shell.execute_reply":"2022-02-02T03:15:51.208814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}